{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewbermingham/PACBayesPINNs/blob/main/PINNs_Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gvpDzEiIUFMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Burgers PINN + PAC-Bayes — Clean Experiments (Colab)\n",
        "# ----------------------------------------------------\n",
        "# - Uses Raissi et al.'s burgers_shock.mat to build a fixed IC/BC pool and dense eval grid\n",
        "# - Deterministic PINN baseline (9×20 tanh) + PAC-Bayes PINN (diagonal Gaussian, PBB-style)\n",
        "# - Reproducible (Nu, Nf) subsampling; relative L2 on dense grid; PAC-Bayes certificate\n",
        "\n",
        "# If SciPy isn't present:  (Colab usually has it)\n",
        "# !pip install scipy --quiet\n",
        "\n",
        "import os, math, time, random, json, urllib.request, gc\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Dict, Tuple, List\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Use float64 (common in PINN repros for stability with L-BFGS)\n",
        "torch.set_default_dtype(torch.float64)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ThWwQk3WLMT",
        "outputId": "e437a2a9-cd9f-4b82-b24e-2258e618d34c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- REPLACE wandb_start with this ---\n",
        "import wandb\n",
        "from dataclasses import asdict\n",
        "\n",
        "def wandb_start(cfg, project, group, name, tags=None):\n",
        "    run = wandb.init(\n",
        "        project=project,\n",
        "        group=group,\n",
        "        name=name,\n",
        "        tags=tags or [],\n",
        "        config=asdict(cfg),\n",
        "        reinit=True,\n",
        "    )\n",
        "    # Make \"epoch\" the shared step axis\n",
        "    wandb.define_metric(\"epoch\")\n",
        "    wandb.define_metric(\"train/*\", step_metric=\"epoch\")\n",
        "    wandb.define_metric(\"lbfgs/*\", step_metric=\"epoch\")\n",
        "    wandb.define_metric(\"pbb/*\", step_metric=\"epoch\")\n",
        "    wandb.define_metric(\"eval/*\", step_metric=\"epoch\")\n",
        "    return run\n"
      ],
      "metadata": {
        "id": "UAOL5tsxa1BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _wandb_images_from_prediction(U_pred, U_ref, t_grid, x_grid, title_prefix=\"\"):\n",
        "    \"\"\"Return a dict of W&B Image objects: heatmap, error heatmap, and 1D slices.\n",
        "       Robust to grids whose max time < 1.0 (e.g., 0.99...).\"\"\"\n",
        "    imgs = {}\n",
        "\n",
        "    # Heatmap of prediction\n",
        "    fig = plt.figure(figsize=(6,3.6))\n",
        "    plt.imshow(U_pred, extent=[x_grid[0], x_grid[-1], t_grid[-1], t_grid[0]],\n",
        "               aspect='auto', origin='upper')\n",
        "    plt.colorbar(); plt.xlabel('x'); plt.ylabel('t')\n",
        "    plt.title(f'{title_prefix} U_pred')\n",
        "    imgs['pred_heatmap'] = wandb.Image(fig); plt.close(fig)\n",
        "\n",
        "    # Heatmap of absolute error\n",
        "    fig = plt.figure(figsize=(6,3.6))\n",
        "    plt.imshow(np.abs(U_pred - U_ref), extent=[x_grid[0], x_grid[-1], t_grid[-1], t_grid[0]],\n",
        "               aspect='auto', origin='upper')\n",
        "    plt.colorbar(); plt.xlabel('x'); plt.ylabel('t')\n",
        "    plt.title(f'{title_prefix} |U_pred - U_ref|')\n",
        "    imgs['error_heatmap'] = wandb.Image(fig); plt.close(fig)\n",
        "\n",
        "    # Robust slices: pick targets within [t_min, t_max] and snap to nearest indices\n",
        "    Nt = U_ref.shape[0]\n",
        "    t_targets = np.linspace(float(t_grid[0]), float(t_grid[-1]), 4)\n",
        "\n",
        "    def nearest_index(val):\n",
        "        return int(np.argmin(np.abs(t_grid - val)))\n",
        "\n",
        "    idxs = [nearest_index(tv) for tv in t_targets]\n",
        "    # Clamp and de-duplicate\n",
        "    idxs = sorted(set(max(0, min(Nt-1, k)) for k in idxs))\n",
        "\n",
        "    fig = plt.figure(figsize=(6,3.6))\n",
        "    for k in idxs:\n",
        "        plt.plot(x_grid, U_ref[k, :], '--', linewidth=1.5, label=f'exact t≈{t_grid[k]:.2f}')\n",
        "        plt.plot(x_grid, U_pred[k, :],  linewidth=1.0, label=f'pred  t≈{t_grid[k]:.2f}')\n",
        "    plt.xlabel('x'); plt.ylabel('u'); plt.title(f'{title_prefix} slices')\n",
        "    plt.legend(ncol=2, fontsize=9)\n",
        "    imgs['slices'] = wandb.Image(fig); plt.close(fig)\n",
        "\n",
        "    return imgs\n",
        "\n"
      ],
      "metadata": {
        "id": "eKP7bCuAa35f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import math\n",
        "import torch\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class PINNConfig:\n",
        "    # --- Domain / PDE ---\n",
        "    t_min: float = 0.0\n",
        "    t_max: float = 1.0\n",
        "    x_min: float = -1.0\n",
        "    x_max: float = 1.0\n",
        "    nu: float = 0.01 / math.pi\n",
        "\n",
        "    # --- Data sizes (new naming) ---\n",
        "    N_ic: int = 100          # total IC+BC points drawn from pool\n",
        "    N_f: int = 10000         # collocation points actually used\n",
        "    Nf_master: int = 20000   # size of fixed master LHS (>= max N_f you’ll sweep)\n",
        "\n",
        "    # --- Network (Raissi baseline: 9 layers total -> 8 hidden × 20) ---\n",
        "    hidden_layers: int = 8\n",
        "    hidden_width: int = 20\n",
        "    activation: str = \"tanh\"\n",
        "\n",
        "    # --- Training / seeds ---\n",
        "    seed: int = 1234\n",
        "\n",
        "    # --- Deterministic PINN (Adam warmup) ---\n",
        "    adam_epochs: int = 2000\n",
        "    adam_lr: float = 1e-3\n",
        "\n",
        "    # --- Bounded loss (kept = 1) ---\n",
        "    loss_cap_B: float = 1.0\n",
        "    loss_type: str = \"clip\"\n",
        "    s_scale: float = 1e-3\n",
        "    alpha: float = 2.2\n",
        "\n",
        "    # in PINNConfig\n",
        "    cert_track_every: int = 200    # evaluate & compare bound every N epochs\n",
        "    cert_track_mc: int = 512       # MC samples for the *tracking* bound during training\n",
        "    save_best_path: Optional[str] = None  # e.g., \"pbb_best_state.pt\" (if None, don’t write to disk)\n",
        "\n",
        "\n",
        "    # For PAC-Bayes PINN objective (clipping budgets and weights)\n",
        "    Bu: float = 1.0\n",
        "    Bf: float = 1.0\n",
        "    lambda_u: float = 0.5\n",
        "    lambda_f: float = 0.5\n",
        "\n",
        "    # --- PAC-Bayes confidence splits ---\n",
        "    delta: float = 0.05\n",
        "    delta_prime: float = 0.01\n",
        "\n",
        "    # --- Certificates / eval ---\n",
        "    eval_mat_path: Optional[str] = \"burgers_shock.mat\"\n",
        "    relL2_mc: int = 64\n",
        "    use_mat_for_data: bool = True\n",
        "\n",
        "    # --- L-BFGS refinement for deterministic baseline ---\n",
        "    use_lbfgs: bool = True\n",
        "    lbfgs_max_iter: int = 10\n",
        "    lbfgs_history_size: int = 50\n",
        "    lbfgs_tolerance_grad: float = 1e-10\n",
        "    lbfgs_tolerance_change: float = 1e-12\n",
        "\n",
        "    # --- Device ---\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # ===========================\n",
        "    # PAC-Bayes training setup (paper-aligned)\n",
        "    # ===========================\n",
        "    prior_sigma0: float = 0.10\n",
        "    pbb_optimizer: str = \"sgdm\"\n",
        "    pbb_lr: float = 1e-3\n",
        "    pbb_epochs: int = 5000\n",
        "    pbb_momentum: float = 0.95\n",
        "    pbb_mc_train: int = 1\n",
        "    cert_mc: int = 150000\n",
        "\n",
        "    mat_path: Optional[str] = \"burgers_shock.mat\"\n",
        "\n",
        "    # --- Optional compatibility aliases so old code that still uses Nu/Nf won’t crash ---\n",
        "    def __post_init__(self):\n",
        "        if not hasattr(self, \"Nu\"):\n",
        "            self.Nu = self.N_ic\n",
        "        if not hasattr(self, \"Nf\"):\n",
        "            self.Nf = self.N_f\n"
      ],
      "metadata": {
        "id": "LiK0OBeeWO9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Utilities\n",
        "# -----------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def ensure_burgers_mat(path: str = \"burgers_shock.mat\") -> str:\n",
        "    if os.path.exists(path):\n",
        "        return path\n",
        "    urls = [\n",
        "        \"https://raw.githubusercontent.com/maziarraissi/PINNs/master/appendix/Data/burgers_shock.mat\",\n",
        "        \"https://github.com/maziarraissi/PINNs/raw/master/appendix/Data/burgers_shock.mat\",\n",
        "    ]\n",
        "    for url in urls:\n",
        "        try:\n",
        "            print(f\"[Data] downloading {url} -> {path}\")\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "            if os.path.exists(path) and os.path.getsize(path) > 0:\n",
        "                return path\n",
        "        except Exception as e:\n",
        "            print(\"Download failed from:\", url, \"error:\", e)\n",
        "    raise FileNotFoundError(\"Could not acquire burgers_shock.mat\")\n",
        "\n",
        "def lhs(n: int, d: int, rng: np.random.Generator) -> np.ndarray:\n",
        "    cut = np.linspace(0, 1, n + 1)\n",
        "    u = rng.random((n, d))\n",
        "    a = cut[:n]\n",
        "    b = cut[1 : n + 1]\n",
        "    rd = u * (b - a)[:, None] + a[:, None]\n",
        "    H = np.zeros_like(rd)\n",
        "    for j in range(d):\n",
        "        order = rng.permutation(n)\n",
        "        H[:, j] = rd[order, 0]\n",
        "    return H\n",
        "\n",
        "def scale_tx_to_unit(tx: torch.Tensor, t_min, t_max, x_min, x_max) -> torch.Tensor:\n",
        "    lb = torch.tensor([t_min, x_min], device=tx.device, dtype=tx.dtype)\n",
        "    ub = torch.tensor([t_max, x_max], device=tx.device, dtype=tx.dtype)\n",
        "    return 2.0 * (tx - lb) / (ub - lb) - 1.0\n"
      ],
      "metadata": {
        "id": "rrDhBVqGWWtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Dataset manager\n",
        "# -----------------------\n",
        "class BurgersData:\n",
        "    \"\"\"\n",
        "    Loads the Raissi .mat, builds:\n",
        "      - pool of IC/BC points X_u_pool (size ≈456) with exact u values\n",
        "      - a master collocation set X_f_master (size Nf_master, then append X_u_pool)\n",
        "      - a high-res eval grid (t_grid, x_grid, U_exact)\n",
        "    Provides deterministic subsampling for any (Nu, Nf) given a seed.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg: PINNConfig):\n",
        "        self.cfg = cfg\n",
        "        set_seed(cfg.seed)\n",
        "\n",
        "        # Load .mat\n",
        "        mat_path = ensure_burgers_mat(\n",
        "        getattr(cfg, \"mat_path\", getattr(cfg, \"eval_mat_path\", \"burgers_shock.mat\"))\n",
        "        )\n",
        "\n",
        "\n",
        "        data = sio.loadmat(mat_path)\n",
        "        t = data[\"t\"].flatten()[:, None]  # (Nt,1)\n",
        "        x = data[\"x\"].flatten()[:, None]  # (Nx,1)\n",
        "        U = np.real(data[\"usol\"]).T       # (Nt, Nx), u(t,x)\n",
        "        self.t_all = t\n",
        "        self.x_all = x\n",
        "        self.U_all = U\n",
        "\n",
        "        # Pool IC/BC from mat grid (Dirichlet boundaries in this dataset)\n",
        "        T, X = np.meshgrid(t, x, indexing=\"ij\")\n",
        "        xx_ic = np.hstack([T[0:1, :].T, X[0:1, :].T])  # (Nx,2) at t=0\n",
        "        uu_ic = U[0:1, :].T\n",
        "\n",
        "        xx_bcL = np.hstack([T[:, 0:1], X[:, 0:1]])    # (Nt,2) at x=-1\n",
        "        uu_bcL = U[:, 0:1]\n",
        "        xx_bcR = np.hstack([T[:, -1:], X[:, -1:]])    # (Nt,2) at x=+1\n",
        "        uu_bcR = U[:, -1:]\n",
        "\n",
        "        X_u_pool = np.vstack([xx_ic, xx_bcL, xx_bcR])\n",
        "        u_pool = np.vstack([uu_ic, uu_bcL, uu_bcR])   # shape (≈456,1)\n",
        "        self.X_u_pool = X_u_pool\n",
        "        self.u_pool = u_pool\n",
        "\n",
        "        # Master collocation design in [t_min,t_max] × [x_min,x_max], then append X_u_pool\n",
        "        rng = np.random.default_rng(cfg.seed)\n",
        "        H = lhs(cfg.Nf_master, 2, rng)\n",
        "        lb = np.array([cfg.t_min, cfg.x_min])\n",
        "        ub = np.array([cfg.t_max, cfg.x_max])\n",
        "        X_f_master = lb + (ub - lb) * H\n",
        "        self.X_f_master = np.vstack([X_f_master, X_u_pool])\n",
        "\n",
        "        # High-res eval grid (default: use mat grid and exact U)\n",
        "        self.eval_t = t[:, 0]\n",
        "        self.eval_x = x[:, 0]\n",
        "        self.eval_U = U  # (Nt, Nx)\n",
        "\n",
        "    def sample_train_sets(self, Nu: int, Nf: int, seed: Optional[int] = None) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Return tensors for (t_ic,x_ic,u_ic), (t_bc,x_bc,u_bc), and (t_f,x_f).\"\"\"\n",
        "        if seed is None: seed = self.cfg.seed\n",
        "        rng = np.random.default_rng(seed)\n",
        "\n",
        "        # Subsample Nu from pool without replacement\n",
        "        poolN = self.X_u_pool.shape[0]\n",
        "        idx = rng.choice(poolN, size=min(Nu, poolN), replace=False)\n",
        "        X_u = self.X_u_pool[idx, :]\n",
        "        u_u = self.u_pool[idx, :]\n",
        "\n",
        "        # Split into IC vs BC via t==0 mask\n",
        "        is_ic = np.isclose(X_u[:, 0], 0.0)\n",
        "        t_ic = torch.tensor(X_u[is_ic, 0:1])\n",
        "        x_ic = torch.tensor(X_u[is_ic, 1:2])\n",
        "        u_ic = torch.tensor(u_u[is_ic, :])\n",
        "\n",
        "        t_bc = torch.tensor(X_u[~is_ic, 0:1])\n",
        "        x_bc = torch.tensor(X_u[~is_ic, 1:2])\n",
        "        u_bc = torch.tensor(u_u[~is_ic, :])\n",
        "\n",
        "        # Collocation: nested subset from master\n",
        "        Xf = self.X_f_master[:Nf, :]\n",
        "        t_f = torch.tensor(Xf[:, 0:1])\n",
        "        x_f = torch.tensor(Xf[:, 1:2])\n",
        "\n",
        "        return {\n",
        "            \"t_ic\": t_ic, \"x_ic\": x_ic, \"u_ic\": u_ic,\n",
        "            \"t_bc\": t_bc, \"x_bc\": x_bc, \"u_bc\": u_bc,\n",
        "            \"t_f\":  t_f,  \"x_f\":  x_f,\n",
        "        }\n",
        "\n",
        "    def eval_grid(self) -> Tuple[torch.Tensor, torch.Tensor, np.ndarray]:\n",
        "        \"\"\"Return (t_grid, x_grid, U_exact) where t_grid shape (Nt,), x_grid shape (Nx,).\"\"\"\n",
        "        t = torch.tensor(self.eval_t)\n",
        "        x = torch.tensor(self.eval_x)\n",
        "        U = self.eval_U\n",
        "        return t, x, U\n",
        "\n",
        "    def rel_l2(self, U_pred: np.ndarray) -> float:\n",
        "        U_ref = self.eval_U\n",
        "        return float(np.linalg.norm(U_pred - U_ref) / np.linalg.norm(U_ref))"
      ],
      "metadata": {
        "id": "q3W_9vhUWc7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# Models and losses\n",
        "# -----------------------\n",
        "class FCNet(nn.Module):\n",
        "    def __init__(self, in_features=2, out_features=1, width=20, depth=8, activation=\"tanh\"):\n",
        "        super().__init__()\n",
        "        acts = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"gelu\": nn.GELU}\n",
        "        Act = acts.get(activation, nn.Tanh)\n",
        "        layers = [nn.Linear(in_features, width), Act()]\n",
        "        for _ in range(depth - 1):\n",
        "            layers += [nn.Linear(width, width), Act()]\n",
        "        layers += [nn.Linear(width, out_features)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.apply(self._init)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, tx_scaled):\n",
        "        return self.net(tx_scaled)\n",
        "\n",
        "def pinn_residual(model: nn.Module, t: torch.Tensor, x: torch.Tensor, cfg: PINNConfig):\n",
        "    t_req = t.clone().detach().requires_grad_(True).to(device)\n",
        "    x_req = x.clone().detach().requires_grad_(True).to(device)\n",
        "    tx = torch.cat([t_req, x_req], dim=1)\n",
        "    tx_s = scale_tx_to_unit(tx, cfg.t_min, cfg.t_max, cfg.x_min, cfg.x_max)\n",
        "\n",
        "    u = model(tx_s)\n",
        "    ones = torch.ones_like(u)\n",
        "    du_dt = torch.autograd.grad(u, t_req, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n",
        "    du_dx = torch.autograd.grad(u, x_req, grad_outputs=ones, retain_graph=True, create_graph=True)[0]\n",
        "    d2u_dx2 = torch.autograd.grad(du_dx, x_req, grad_outputs=torch.ones_like(du_dx), retain_graph=True, create_graph=True)[0]\n",
        "\n",
        "    f = du_dt + u * du_dx - cfg.nu * d2u_dx2\n",
        "    return f\n",
        "\n",
        "def pinn_loss(model: nn.Module, batch: Dict[str, torch.Tensor], cfg: PINNConfig) -> Tuple[torch.Tensor, Dict]:\n",
        "    # Data terms\n",
        "    def _pred(t, x):\n",
        "        tx = torch.cat([t.to(device), x.to(device)], dim=1)\n",
        "        tx_s = scale_tx_to_unit(tx, cfg.t_min, cfg.t_max, cfg.x_min, cfg.x_max)\n",
        "        return model(tx_s)\n",
        "\n",
        "    u_ic_pred = _pred(batch[\"t_ic\"], batch[\"x_ic\"]) if batch[\"t_ic\"].numel()>0 else torch.tensor([], device=device)\n",
        "    u_bc_pred = _pred(batch[\"t_bc\"], batch[\"x_bc\"]) if batch[\"t_bc\"].numel()>0 else torch.tensor([], device=device)\n",
        "\n",
        "    loss_ic = torch.mean((u_ic_pred - batch[\"u_ic\"].to(device))**2) if batch[\"t_ic\"].numel()>0 else torch.tensor(0., device=device)\n",
        "    loss_bc = torch.mean((u_bc_pred - batch[\"u_bc\"].to(device))**2) if batch[\"t_bc\"].numel()>0 else torch.tensor(0., device=device)\n",
        "\n",
        "    # Physics residual\n",
        "    f_pred = pinn_residual(model, batch[\"t_f\"], batch[\"x_f\"], cfg)\n",
        "    loss_f = torch.mean(f_pred**2)\n",
        "\n",
        "    loss = loss_ic + loss_bc + loss_f\n",
        "    return loss, {\"ic\": float(loss_ic.detach().cpu()), \"bc\": float(loss_bc.detach().cpu()), \"f\": float(loss_f.detach().cpu())}"
      ],
      "metadata": {
        "id": "nUrBOizfWg3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- REPLACE train_pinn with this ---\n",
        "def train_pinn(cfg: PINNConfig, data: BurgersData,\n",
        "               Nu: Optional[int]=None, Nf: Optional[int]=None,\n",
        "               verbose=True,\n",
        "               log_wandb: bool=False,\n",
        "               wb_project: str=\"pinn-burgers\",\n",
        "               log_every: int = 100):\n",
        "\n",
        "    # use new config fields by default\n",
        "    if Nu is None: Nu = cfg.N_ic\n",
        "    if Nf is None: Nf = cfg.N_f\n",
        "\n",
        "    set_seed(cfg.seed)\n",
        "    batch = data.sample_train_sets(Nu, Nf, seed=cfg.seed)\n",
        "    model = FCNet(width=cfg.hidden_width, depth=cfg.hidden_layers, activation=cfg.activation).to(device)\n",
        "    opt = Adam(model.parameters(), lr=cfg.adam_lr)\n",
        "\n",
        "    run = None\n",
        "    if log_wandb:\n",
        "        run = wandb_start(cfg, wb_project, group=\"deterministic\",\n",
        "                          name=f\"PINN_Nu{Nu}_Nf{Nf}_seed{cfg.seed}\",\n",
        "                          tags=[f\"Nu={Nu}\", f\"Nf={Nf}\", \"baseline\"])\n",
        "\n",
        "    t0 = time.time()\n",
        "    for ep in range(1, cfg.adam_epochs + 1):\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss, parts = pinn_loss(model, batch, cfg)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        if (ep % log_every == 0) or (ep == 1):\n",
        "            if verbose:\n",
        "                print(f\"[Adam] ep={ep:4d} loss={loss.item():.3e} ic={parts['ic']:.3e} bc={parts['bc']:.3e} f={parts['f']:.3e}\")\n",
        "            if run:\n",
        "                wandb.log({\n",
        "                    \"train/loss\": float(loss.item()),\n",
        "                    \"train/ic\": parts['ic'],\n",
        "                    \"train/bc\": parts['bc'],\n",
        "                    \"train/f\":  parts['f'],\n",
        "                    \"epoch\": ep\n",
        "                })\n",
        "\n",
        "    if cfg.use_lbfgs:\n",
        "        lbfgs = torch.optim.LBFGS(\n",
        "            model.parameters(),\n",
        "            lr=1.0,\n",
        "            max_iter=cfg.lbfgs_max_iter,\n",
        "            max_eval=cfg.lbfgs_max_iter,\n",
        "            tolerance_grad=cfg.lbfgs_tolerance_grad,\n",
        "            tolerance_change=cfg.lbfgs_tolerance_change,\n",
        "            history_size=cfg.lbfgs_history_size,\n",
        "            line_search_fn='strong_wolfe'\n",
        "        )\n",
        "        def closure():\n",
        "            lbfgs.zero_grad(set_to_none=True)\n",
        "            l, _ = pinn_loss(model, batch, cfg)\n",
        "            l.backward()\n",
        "            return l\n",
        "        l0 = closure().item()\n",
        "        lbfgs.step(closure)\n",
        "        l1, parts1 = pinn_loss(model, batch, cfg)\n",
        "        if verbose:\n",
        "            print(f\"[LBFGS] before={l0:.3e} after={l1.item():.3e} ic={parts1['ic']:.3e} bc={parts1['bc']:.3e} f={parts1['f']:.3e}\")\n",
        "        if run:\n",
        "            wandb.log({\n",
        "                \"lbfgs/loss_before\": float(l0),\n",
        "                \"lbfgs/loss_after\": float(l1.item()),\n",
        "                \"lbfgs/ic\": parts1['ic'],\n",
        "                \"lbfgs/bc\": parts1['bc'],\n",
        "                \"lbfgs/f\":  parts1['f'],\n",
        "                \"epoch\": cfg.adam_epochs  # final Adam epoch index\n",
        "            })\n",
        "\n",
        "    # Dense-grid evaluation AFTER L-BFGS\n",
        "    t_grid, x_grid, U_exact = data.eval_grid()\n",
        "    TT, XX = np.meshgrid(t_grid.numpy(), x_grid.numpy(), indexing='ij')\n",
        "    tx = torch.tensor(np.stack([TT.ravel(), XX.ravel()], axis=1), dtype=torch.get_default_dtype(), device=device)\n",
        "    with torch.no_grad():\n",
        "        U_pred = model(scale_tx_to_unit(tx, cfg.t_min, cfg.t_max, cfg.x_min, cfg.x_max)).cpu().numpy().reshape(U_exact.shape)\n",
        "\n",
        "    rel = data.rel_l2(U_pred)\n",
        "    elapsed = time.time() - t0\n",
        "    if verbose:\n",
        "        print(f\"[Eval] rel-L2={rel:.3e} | time={elapsed:.1f}s\")\n",
        "\n",
        "    if run:\n",
        "        imgs = _wandb_images_from_prediction(U_pred, U_exact, t_grid.numpy(), x_grid.numpy(), title_prefix=\"PINN\")\n",
        "        wandb.log({\"eval/relL2\": rel, \"time/seconds\": elapsed, **imgs, \"epoch\": cfg.adam_epochs})\n",
        "\n",
        "        run.finish()\n",
        "    return model, rel, {\"time\": elapsed}\n"
      ],
      "metadata": {
        "id": "lQf3grwLWmqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IAqps4fUHnXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# PAC-Bayes PINN (Diagonal Gaussian posterior)\n",
        "# -----------------------\n",
        "# --- REPLACE BayesLinear with this ---\n",
        "class BayesLinear(nn.Module):\n",
        "    def __init__(self, in_f, out_f, prior_sigma0=1.0):\n",
        "        super().__init__()\n",
        "        self.prior_sigma0 = float(prior_sigma0)\n",
        "\n",
        "        # posterior parameters\n",
        "        self.mu_w = nn.Parameter(torch.empty(out_f, in_f))\n",
        "        self.mu_b = nn.Parameter(torch.empty(out_f))\n",
        "        rho0 = math.log(math.expm1(prior_sigma0))\n",
        "        self.rho_w = nn.Parameter(torch.full((out_f, in_f), rho0))\n",
        "        self.rho_b = nn.Parameter(torch.full((out_f,), rho0))\n",
        "\n",
        "        # prior centers μ0 (buffers; fixed during PAC-Bayes training)\n",
        "        self.register_buffer(\"mu0_w\", torch.empty(out_f, in_f))\n",
        "        self.register_buffer(\"mu0_b\", torch.empty(out_f))\n",
        "\n",
        "        # truncated N(0, 1/√n_in), clipped at ±2σ\n",
        "        std = 1.0 / math.sqrt(in_f)\n",
        "        with torch.no_grad():\n",
        "            self.mu0_w.copy_(truncated_normal_like(self.mu0_w, std))\n",
        "            self.mu0_b.copy_(truncated_normal_like(self.mu0_b, std))\n",
        "            # posterior initialised at the prior (centers & scales)\n",
        "            self.mu_w.copy_(self.mu0_w)\n",
        "            self.mu_b.copy_(self.mu0_b)\n",
        "\n",
        "    @property\n",
        "    def sigma_w(self): return F.softplus(self.rho_w)\n",
        "    @property\n",
        "    def sigma_b(self): return F.softplus(self.rho_b)\n",
        "\n",
        "    def sample_params(self):\n",
        "        eps_w = torch.randn_like(self.mu_w)\n",
        "        eps_b = torch.randn_like(self.mu_b)\n",
        "        W = self.mu_w + self.sigma_w * eps_w\n",
        "        b = self.mu_b + self.sigma_b * eps_b\n",
        "        return W, b\n",
        "\n",
        "    def mean_params(self):\n",
        "        return self.mu_w, self.mu_b\n",
        "    def kl(self):\n",
        "        s0 = self.prior_sigma0\n",
        "        s02 = s0 * s0\n",
        "        def _kl(mu, mu0, sigma):\n",
        "            return 0.5 * torch.sum((sigma**2)/s02 + ((mu - mu0)**2)/s02 - 1.0 + torch.log(s02/(sigma**2 + 1e-12)))\n",
        "        return _kl(self.mu_w, self.mu0_w, self.sigma_w) + _kl(self.mu_b, self.mu0_b, self.sigma_b)\n",
        "\n",
        "def truncated_normal_like(t, std):\n",
        "    out = torch.empty_like(t)\n",
        "    ok = torch.zeros_like(out, dtype=torch.bool)\n",
        "    while not torch.all(ok):\n",
        "        s = torch.randn_like(out) * std\n",
        "        mask = (s.abs() <= 2*std)\n",
        "        out = torch.where(ok, out, s)\n",
        "        ok = ok | mask\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "# --- REPLACE BayesFCNet with this ---\n",
        "class BayesFCNet(nn.Module):\n",
        "    def __init__(self, in_features=2, out_features=1, width=20, depth=8, activation=\"tanh\", prior_sigma0=1.0):\n",
        "        super().__init__()\n",
        "        acts = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"gelu\": nn.GELU}\n",
        "        Act = acts.get(activation, nn.Tanh)\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.acts = nn.ModuleList([Act() for _ in range(depth)])\n",
        "        # input\n",
        "        self.layers.append(BayesLinear(in_features, width, prior_sigma0))\n",
        "        # hidden\n",
        "        for _ in range(depth - 1):\n",
        "            self.layers.append(BayesLinear(width, width, prior_sigma0))\n",
        "        # output\n",
        "        self.out = BayesLinear(width, out_features, prior_sigma0)\n",
        "\n",
        "    # ---- NEW: sample once and reuse ----\n",
        "    def sample_weights(self):\n",
        "        weights = []\n",
        "        for layer in self.layers:\n",
        "            W, b = layer.sample_params()\n",
        "            weights.append((W, b))\n",
        "        W, b = self.out.sample_params()\n",
        "        out_w = (W, b)\n",
        "        return weights, out_w\n",
        "\n",
        "    # ---- NEW: KL once per step ----\n",
        "    def kl_total(self):\n",
        "        kl = torch.tensor(0.0, device=self.layers[0].mu_w.device)\n",
        "        for layer in self.layers:\n",
        "            kl = kl + layer.kl()\n",
        "        kl = kl + self.out.kl()\n",
        "        return kl\n",
        "\n",
        "    # ---- NEW: deterministic forward with provided weights ----\n",
        "    def forward_with_weights(self, tx_scaled, weights, out_w):\n",
        "        h = tx_scaled\n",
        "        for i, ((W, b), act) in enumerate(zip(weights, self.acts)):\n",
        "            h = h @ W.T + b\n",
        "            h = act(h)\n",
        "        W, b = out_w\n",
        "        y = h @ W.T + b\n",
        "        return y\n",
        "\n",
        "    # (keep these two for convenience)\n",
        "    def forward_sample(self, tx_scaled):\n",
        "        weights, out_w = self.sample_weights()\n",
        "        y = self.forward_with_weights(tx_scaled, weights, out_w)\n",
        "        return y, self.kl_total()\n",
        "\n",
        "    def forward_mean(self, tx_scaled):\n",
        "        h = tx_scaled\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            W, b = layer.mean_params()\n",
        "            h = h @ W.T + b\n",
        "            h = self.acts[i](h)\n",
        "        W, b = self.out.mean_params()\n",
        "        y = h @ W.T + b\n",
        "        return y\n",
        "\n",
        "\n",
        "def bounded_mse(y, y_true, B):\n",
        "    return torch.clamp((y - y_true)**2, max=B) / B\n",
        "\n",
        "# -----------------------\n",
        "# PAC-Bayes PINN (Diagonal Gaussian posterior)\n",
        "# -----------------------\n",
        "# --- REPLACE BayesLinear with this ---\n",
        "class BayesLinear(nn.Module):\n",
        "    def __init__(self, in_f, out_f, prior_sigma0=1.0):\n",
        "        super().__init__()\n",
        "        self.prior_sigma0 = float(prior_sigma0)\n",
        "\n",
        "        # posterior parameters\n",
        "        self.mu_w = nn.Parameter(torch.empty(out_f, in_f))\n",
        "        self.mu_b = nn.Parameter(torch.empty(out_f))\n",
        "        rho0 = math.log(math.expm1(prior_sigma0))\n",
        "        self.rho_w = nn.Parameter(torch.full((out_f, in_f), rho0))\n",
        "        self.rho_b = nn.Parameter(torch.full((out_f,), rho0))\n",
        "\n",
        "        # prior centers μ0 (buffers; fixed during PAC-Bayes training)\n",
        "        self.register_buffer(\"mu0_w\", torch.empty(out_f, in_f))\n",
        "        self.register_buffer(\"mu0_b\", torch.empty(out_f))\n",
        "\n",
        "        # truncated N(0, 1/√n_in), clipped at ±2σ\n",
        "        std = 1.0 / math.sqrt(in_f)\n",
        "        with torch.no_grad():\n",
        "            self.mu0_w.copy_(truncated_normal_like(self.mu0_w, std))\n",
        "            self.mu0_b.copy_(truncated_normal_like(self.mu0_b, std))\n",
        "            # posterior initialised at the prior (centers & scales)\n",
        "            self.mu_w.copy_(self.mu0_w)\n",
        "            self.mu_b.copy_(self.mu0_b)\n",
        "\n",
        "    @property\n",
        "    def sigma_w(self): return F.softplus(self.rho_w)\n",
        "    @property\n",
        "    def sigma_b(self): return F.softplus(self.rho_b)\n",
        "\n",
        "    def sample_params(self):\n",
        "        eps_w = torch.randn_like(self.mu_w)\n",
        "        eps_b = torch.randn_like(self.mu_b)\n",
        "        W = self.mu_w + self.sigma_w * eps_w\n",
        "        b = self.mu_b + self.sigma_b * eps_b\n",
        "        return W, b\n",
        "\n",
        "    def mean_params(self):\n",
        "        return self.mu_w, self.mu_b\n",
        "    def kl(self):\n",
        "        s0 = self.prior_sigma0\n",
        "        s02 = s0 * s0\n",
        "        def _kl(mu, mu0, sigma):\n",
        "            return 0.5 * torch.sum((sigma**2)/s02 + ((mu - mu0)**2)/s02 - 1.0 + torch.log(s02/(sigma**2 + 1e-12)))\n",
        "        return _kl(self.mu_w, self.mu0_w, self.sigma_w) + _kl(self.mu_b, self.mu0_b, self.sigma_b)\n",
        "\n",
        "def truncated_normal_like(t, std):\n",
        "    out = torch.empty_like(t)\n",
        "    ok = torch.zeros_like(out, dtype=torch.bool)\n",
        "    while not torch.all(ok):\n",
        "        s = torch.randn_like(out) * std\n",
        "        mask = (s.abs() <= 2*std)\n",
        "        out = torch.where(ok, out, s)\n",
        "        ok = ok | mask\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "# --- REPLACE BayesFCNet with this ---\n",
        "class BayesFCNet(nn.Module):\n",
        "    def __init__(self, in_features=2, out_features=1, width=20, depth=8, activation=\"tanh\", prior_sigma0=1.0):\n",
        "        super().__init__()\n",
        "        acts = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"gelu\": nn.GELU}\n",
        "        Act = acts.get(activation, nn.Tanh)\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.acts = nn.ModuleList([Act() for _ in range(depth)])\n",
        "        # input\n",
        "        self.layers.append(BayesLinear(in_features, width, prior_sigma0))\n",
        "        # hidden\n",
        "        for _ in range(depth - 1):\n",
        "            self.layers.append(BayesLinear(width, width, prior_sigma0))\n",
        "        # output\n",
        "        self.out = BayesLinear(width, out_features, prior_sigma0)\n",
        "\n",
        "    # ---- NEW: sample once and reuse ----\n",
        "    def sample_weights(self):\n",
        "        weights = []\n",
        "        for layer in self.layers:\n",
        "            W, b = layer.sample_params()\n",
        "            weights.append((W, b))\n",
        "        W, b = self.out.sample_params()\n",
        "        out_w = (W, b)\n",
        "        return weights, out_w\n",
        "\n",
        "    # ---- NEW: KL once per step ----\n",
        "    def kl_total(self):\n",
        "        kl = torch.tensor(0.0, device=self.layers[0].mu_w.device)\n",
        "        for layer in self.layers:\n",
        "            kl = kl + layer.kl()\n",
        "        kl = kl + self.out.kl()\n",
        "        return kl\n",
        "\n",
        "    # ---- NEW: deterministic forward with provided weights ----\n",
        "    def forward_with_weights(self, tx_scaled, weights, out_w):\n",
        "        h = tx_scaled\n",
        "        for i, ((W, b), act) in enumerate(zip(weights, self.acts)):\n",
        "            h = h @ W.T + b\n",
        "            h = act(h)\n",
        "        W, b = out_w\n",
        "        y = h @ W.T + b\n",
        "        return y\n",
        "\n",
        "    # (keep these two for convenience)\n",
        "    def forward_sample(self, tx_scaled):\n",
        "        weights, out_w = self.sample_weights()\n",
        "        y = self.forward_with_weights(tx_scaled, weights, out_w)\n",
        "        return y, self.kl_total()\n",
        "\n",
        "    def forward_mean(self, tx_scaled):\n",
        "        h = tx_scaled\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            W, b = layer.mean_params()\n",
        "            h = h @ W.T + b\n",
        "            h = self.acts[i](h)\n",
        "        W, b = self.out.mean_params()\n",
        "        y = h @ W.T + b\n",
        "        return y\n",
        "\n",
        "\n",
        "def bounded_mse(y, y_true, B):\n",
        "    return torch.clamp((y - y_true)**2, max=B) / B\n",
        "\n",
        "# MODIFIED FUNCTION\n",
        "def pbb_objective(modelB: BayesFCNet, batch, cfg: PINNConfig, mc_samples: int = 1): # <-- REMOVED eval_mode\n",
        "    Bu = cfg.Bu\n",
        "    Bf = cfg.Bf\n",
        "    lam_u = cfg.lambda_u\n",
        "    lam_f = cfg.lambda_f\n",
        "\n",
        "    m_u = int(batch[\"t_ic\"].shape[0] + batch[\"t_bc\"].shape[0])\n",
        "    m_f = int(batch[\"t_f\"].shape[0])\n",
        "    cu = math.log(4.0*math.sqrt(max(1, m_u)) / cfg.delta)\n",
        "    cf = math.log(4.0*math.sqrt(max(1, m_f)) / cfg.delta)\n",
        "\n",
        "    # scale inputs\n",
        "    tx_ic_s = None; tx_bc_s = None\n",
        "    if m_u > 0:\n",
        "        tx_ic = torch.cat([batch[\"t_ic\"].to(device), batch[\"x_ic\"].to(device)], dim=1)\n",
        "        tx_bc = torch.cat([batch[\"t_bc\"].to(device), batch[\"x_bc\"].to(device)], dim=1)\n",
        "        tx_ic_s = scale_tx_to_unit(tx_ic, cfg.t_min, cfg.t_max, cfg.x_min, cfg.x_max)\n",
        "        tx_bc_s = scale_tx_to_unit(tx_bc, cfg.t_min, cfg.t_max, cfg.x_min, cfg.x_max)\n",
        "\n",
        "    # residual inputs with grads\n",
        "    t_f = batch[\"t_f\"].clone().detach().requires_grad_(True).to(device)\n",
        "    x_f = batch[\"x_f\"].clone().detach().requires_grad_(True).to(device)\n",
        "    tx_f_s = scale_tx_to_unit(torch.cat([t_f, x_f], dim=1), cfg.t_min, cfg.t_max, cfg.x_min, cfg.x_max)\n",
        "\n",
        "    # --- RESTORED a single, safe grad flag configuration ---\n",
        "    cg_dt, rg_dt = True,  True\n",
        "    cg_dx, rg_dx = True,  True\n",
        "    cg_d2, rg_d2 = True,  True\n",
        "\n",
        "    Lu_acc = 0.0; Lf_acc = 0.0; KL_acc = 0.0\n",
        "    frac_clip_u_acc = 0.0; frac_clip_f_acc = 0.0\n",
        "\n",
        "    for _ in range(mc_samples):\n",
        "        weights, out_w = modelB.sample_weights()\n",
        "        KL = modelB.kl_total()\n",
        "\n",
        "        # bounded data term\n",
        "        if m_u > 0:\n",
        "            y_ic = modelB.forward_with_weights(tx_ic_s, weights, out_w)\n",
        "            y_bc = modelB.forward_with_weights(tx_bc_s, weights, out_w)\n",
        "            sq_ic = (y_ic - batch[\"u_ic\"].to(device))**2\n",
        "            sq_bc = (y_bc - batch[\"u_bc\"].to(device))**2\n",
        "            Lu = (torch.clamp(sq_ic, max=Bu)/Bu).mean() + (torch.clamp(sq_bc, max=Bu)/Bu).mean()\n",
        "            Lu = 0.5 * Lu\n",
        "            frac_clip_u = 0.5*((sq_ic >= Bu).float().mean() + (sq_bc >= Bu).float().mean())\n",
        "        else:\n",
        "            Lu = torch.tensor(0.0, device=device)\n",
        "            frac_clip_u = torch.tensor(0.0, device=device)\n",
        "\n",
        "        # bounded residual term\n",
        "        y_f = modelB.forward_with_weights(tx_f_s, weights, out_w)\n",
        "        du_dt = torch.autograd.grad(y_f, t_f, grad_outputs=torch.ones_like(y_f),\n",
        "                                    retain_graph=rg_dt, create_graph=cg_dt)[0]\n",
        "        du_dx = torch.autograd.grad(y_f, x_f, grad_outputs=torch.ones_like(y_f),\n",
        "                                    retain_graph=rg_dx, create_graph=cg_dx)[0]\n",
        "        d2u_dx2 = torch.autograd.grad(du_dx, x_f, grad_outputs=torch.ones_like(du_dx),\n",
        "                                      retain_graph=rg_d2, create_graph=cg_d2)[0]\n",
        "        f = du_dt + y_f * du_dx - cfg.nu * d2u_dx2\n",
        "\n",
        "        sq_f = f**2\n",
        "        Lf = torch.clamp(sq_f, max=Bf).mean() / Bf\n",
        "        frac_clip_f = (sq_f >= Bf).float().mean()\n",
        "\n",
        "        Lu_acc += Lu; Lf_acc += Lf; KL_acc += KL\n",
        "        frac_clip_u_acc += frac_clip_u; frac_clip_f_acc += frac_clip_f\n",
        "\n",
        "    Lu_bar = Lu_acc / mc_samples\n",
        "    Lf_bar = Lf_acc / mc_samples\n",
        "    KL_bar = KL_acc / mc_samples\n",
        "    frac_clip_u_bar = frac_clip_u_acc / mc_samples\n",
        "    frac_clip_f_bar = frac_clip_f_acc / mc_samples\n",
        "\n",
        "    term_u = lam_u * torch.sqrt(torch.clamp((KL_bar + cu) / (2.0*max(1, m_u)), min=0.0))\n",
        "    term_f = lam_f * torch.sqrt(torch.clamp((KL_bar + cf) / (2.0*max(1, m_f)), min=0.0))\n",
        "\n",
        "    Lpin = lam_u*Lu_bar + lam_f*Lf_bar\n",
        "    J = Lpin + term_u + term_f\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mse_u = torch.tensor(0.0, device=device)\n",
        "        if m_u > 0:\n",
        "            mse_u = 0.5*((y_ic - batch[\"u_ic\"].to(device))**2).mean() + 0.5*((y_bc - batch[\"u_bc\"].to(device))**2).mean()\n",
        "        mse_f = (f**2).mean()\n",
        "\n",
        "    return J, {\n",
        "        \"Lu\": float(Lu_bar.detach().cpu()),\n",
        "        \"Lf\": float(Lf_bar.detach().cpu()),\n",
        "        \"KL\": float(KL_bar.detach().cpu()),\n",
        "        \"term_u\": float(term_u.detach().cpu()),\n",
        "        \"term_f\": float(term_f.detach().cpu()),\n",
        "        \"clip_frac_u\": float(frac_clip_u_bar.detach().cpu()),\n",
        "        \"clip_frac_f\": float(frac_clip_f_bar.detach().cpu()),\n",
        "        \"mse_u\": float(mse_u.detach().cpu()),\n",
        "        \"mse_f\": float(mse_f.detach().cpu()),\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _coarse_relL2_mean(modelB, data: BurgersData, cfg: PINNConfig, stride_t=4, stride_x=4):\n",
        "    \"\"\"Cheaper periodic check: rel-L2 on a coarser grid (no MC).\"\"\"\n",
        "    t_grid, x_grid, U_exact = data.eval_grid()\n",
        "    t_sub = t_grid[::stride_t]; x_sub = x_grid[::stride_x]\n",
        "    U_sub  = U_exact[::stride_t, ::stride_x]\n",
        "    TT, XX = np.meshgrid(t_sub.numpy(), x_sub.numpy(), indexing='ij')\n",
        "    tx = torch.tensor(np.stack([TT.ravel(), XX.ravel()], axis=1), dtype=torch.get_default_dtype(), device=device)\n",
        "    with torch.no_grad():\n",
        "        U_mean = modelB.forward_mean(scale_tx_to_unit(tx, cfg.t_min, cfg.t_max, cfg.x_min, cfg.x_max)).cpu().numpy().reshape(U_sub.shape)\n",
        "    return float(np.linalg.norm(U_mean - U_sub) / np.linalg.norm(U_sub))\n",
        "\n",
        "def _posterior_sigma_stats(modelB: 'BayesFCNet'):\n",
        "    stats = {}\n",
        "    L = len(modelB.layers)\n",
        "    for i, layer in enumerate(list(modelB.layers) + [modelB.out]):\n",
        "        sw = layer.sigma_w.detach().cpu().numpy().ravel()\n",
        "        sb = layer.sigma_b.detach().cpu().numpy().ravel()\n",
        "        stats[f\"sigma/mean_w_L{i}\"] = float(np.mean(sw))\n",
        "        stats[f\"sigma/max_w_L{i}\"]  = float(np.max(sw))\n",
        "        stats[f\"sigma/mean_b_L{i}\"] = float(np.mean(sb))\n",
        "        stats[f\"sigma/max_b_L{i}\"]  = float(np.max(sb))\n",
        "    return stats\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jLpzpp6QWsgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, Callable\n",
        "\n",
        "@torch.no_grad()\n",
        "def calculate_certificate_batched(modelB, batch, cfg, total_mc, batch_size=64):\n",
        "    \"\"\"\n",
        "    Calculates the PBB objective by processing MC samples in batches to conserve memory.\n",
        "    This prevents OutOfMemoryError during the final high-sample certificate calculation.\n",
        "    \"\"\"\n",
        "    if total_mc == 0:\n",
        "        return 0.0\n",
        "\n",
        "    J_total = 0.0\n",
        "    num_batches = (total_mc + batch_size - 1) // batch_size\n",
        "    print(f\"Calculating certificate with {total_mc} samples in {num_batches} batches of size {batch_size}...\")\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        mc_samples_in_batch = min(batch_size, total_mc - i * batch_size)\n",
        "\n",
        "        # Gradients must be enabled here because the pbb_objective function\n",
        "        # needs them to compute the PDE residual via autograd.\n",
        "        with torch.enable_grad():\n",
        "            J_batch, _ = pbb_objective(modelB, batch, cfg, mc_samples=mc_samples_in_batch)\n",
        "\n",
        "        # Accumulate the result and immediately free memory\n",
        "        J_total += J_batch.item() * mc_samples_in_batch\n",
        "        del J_batch\n",
        "        _gpu_hygiene() # A helper function to call gc.collect() and torch.cuda.empty_cache()\n",
        "\n",
        "    # Return the final average over all Monte Carlo samples\n",
        "    return J_total / total_mc\n",
        "\n",
        "\n",
        "# --- REPLACE your old train_pacbayes with this version ---\n",
        "def train_pacbayes(cfg: PINNConfig, data: BurgersData,\n",
        "                   Nu: Optional[int]=None, Nf: Optional[int]=None,\n",
        "                   verbose=True,\n",
        "                   log_wandb: bool=False,\n",
        "                   wb_project: str=\"pinn-burgers\",\n",
        "                   log_every: int = 100,\n",
        "                   epochs: Optional[int] = None,\n",
        "                   mem_check_fn: Optional[Callable] = None): # Optional: for memory debugging\n",
        "    # sizes\n",
        "    if Nu is None: Nu = cfg.N_ic\n",
        "    if Nf is None: Nf = cfg.N_f\n",
        "    if epochs is None:\n",
        "        epochs = getattr(cfg, \"pbb_epochs\", 5000)\n",
        "\n",
        "    track_every = getattr(cfg, \"cert_track_every\", 200)\n",
        "    track_mc = getattr(cfg, \"cert_track_mc\", 64)\n",
        "    save_path = getattr(cfg, \"save_best_path\", None)\n",
        "    relL2_mc = getattr(cfg, \"relL2_mc\", 64)\n",
        "\n",
        "    import copy\n",
        "    import torch as _torch\n",
        "\n",
        "    set_seed(cfg.seed)\n",
        "    batch = data.sample_train_sets(Nu, Nf, seed=cfg.seed)\n",
        "\n",
        "    modelB = BayesFCNet(width=cfg.hidden_width, depth=cfg.hidden_layers,\n",
        "                        activation=cfg.activation, prior_sigma0=cfg.prior_sigma0).to(device)\n",
        "\n",
        "    opt = torch.optim.SGD(modelB.parameters(), lr=cfg.pbb_lr, momentum=cfg.pbb_momentum)\n",
        "\n",
        "    run = None\n",
        "    if log_wandb:\n",
        "        run = wandb_start(cfg, wb_project, group=\"pacbayes\",\n",
        "                          name=f\"PBB_Nu{Nu}_Nf{Nf}_seed{cfg.seed}\",\n",
        "                          tags=[f\"Nu={Nu}\", f\"Nf={Nf}\", \"pbb\"])\n",
        "\n",
        "    best_est_cert = float(\"inf\")\n",
        "    best_epoch = -1\n",
        "    best_state = None\n",
        "\n",
        "    t0 = time.time()\n",
        "    for ep in range(1, epochs + 1):\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        J, parts = pbb_objective(modelB, batch, cfg, mc_samples=cfg.pbb_mc_train)\n",
        "        J.backward()\n",
        "        opt.step()\n",
        "\n",
        "        if (ep % log_every == 0) or (ep == 1):\n",
        "            if verbose:\n",
        "                print(f\"[PBB] ep={ep:4d} J={J.item():.3e} Lu={parts['Lu']:.3e} Lf={parts['Lf']:.3e} \"\n",
        "                      f\"KL={parts['KL']:.3e} tu={parts.get('term_u',0):.3e} tf={parts.get('term_f',0):.3e} \"\n",
        "                      f\"clip_u={parts.get('clip_frac_u',0):.2f} clip_f={parts.get('clip_frac_f',0):.2f}\")\n",
        "            if run:\n",
        "                # Ensure all parts are included in the log\n",
        "                log_data = {\"pbb/J\": J.item(), \"epoch\": ep}\n",
        "                for k, v in parts.items():\n",
        "                    log_data[f\"pbb/{k}\"] = v\n",
        "                wandb.log(log_data)\n",
        "\n",
        "\n",
        "        if (ep % track_every == 0) or (ep == 1) or (ep == epochs):\n",
        "            with torch.enable_grad():\n",
        "                J_est, _ = pbb_objective(modelB, batch, cfg, mc_samples=track_mc)\n",
        "            J_est_val = float(J_est.detach().cpu())\n",
        "            del J_est; _gpu_hygiene()\n",
        "\n",
        "            if run:\n",
        "                wandb.log({\"cert/track_estimate\": J_est_val, \"epoch\": ep})\n",
        "            if J_est_val < best_est_cert:\n",
        "                best_est_cert = J_est_val\n",
        "                best_epoch = ep\n",
        "                state_gpu = modelB.state_dict()\n",
        "                best_state = {k: v.detach().cpu().clone() for k, v in state_gpu.items()}\n",
        "                del state_gpu; _gpu_hygiene()\n",
        "                if run:\n",
        "                    wandb.log({\"cert/best_track_estimate\": best_est_cert,\n",
        "                               \"cert/best_epoch\": best_epoch,\n",
        "                               \"epoch\": ep})\n",
        "\n",
        "    if best_state is not None:\n",
        "        modelB.load_state_dict(best_state)\n",
        "        if save_path:\n",
        "            torch.save(best_state, save_path)\n",
        "            if run: wandb.save(save_path)\n",
        "\n",
        "    # --- MINIMAL CHANGE IS HERE ---\n",
        "    # Replace the single, memory-intensive call with our new batched function.\n",
        "    if mem_check_fn: mem_check_fn(\"PBB - Before Certificate\")\n",
        "    cert = calculate_certificate_batched(\n",
        "        modelB, batch, cfg,\n",
        "        total_mc=cfg.cert_mc,\n",
        "        batch_size=64\n",
        "    )\n",
        "    if mem_check_fn: mem_check_fn(\"PBB - After Certificate\")\n",
        "    # --- END OF MINIMAL CHANGE ---\n",
        "\n",
        "    t_grid, x_grid, U_exact = data.eval_grid()\n",
        "    TT, XX = np.meshgrid(t_grid.numpy(), x_grid.numpy(), indexing='ij')\n",
        "    tx = torch.tensor(np.stack([TT.ravel(), XX.ravel()], axis=1), dtype=torch.get_default_dtype(), device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        U_mean = modelB.forward_mean(scale_tx_to_unit(tx, cfg.t_min, cfg.t_max, cfg.x_min, cfg.x_max)).cpu().numpy().reshape(U_exact.shape)\n",
        "    rel_mean = data.rel_l2(U_mean)\n",
        "\n",
        "    rel_samples = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(min(relL2_mc, cfg.cert_mc)):\n",
        "            weights, out_w = modelB.sample_weights()\n",
        "            U_s = modelB.forward_with_weights(scale_tx_to_unit(tx, cfg.t_min, cfg.t_max, cfg.x_min, cfg.x_max),\n",
        "                                              weights, out_w).cpu().numpy().reshape(U_exact.shape)\n",
        "            rel_samples.append(data.rel_l2(U_s))\n",
        "    rel_stoch = float(np.mean(rel_samples)) if rel_samples else float('nan')\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    if verbose:\n",
        "        print(f\"[PBB Eval] cert={cert:.3e} relL2_mean={rel_mean:.3e} relL2_stoch={rel_stoch:.3e} time={elapsed:.1f}s\")\n",
        "\n",
        "    if run:\n",
        "        imgs = _wandb_images_from_prediction(U_mean, U_exact, t_grid.numpy(), x_grid.numpy(), title_prefix=\"PBB mean\")\n",
        "        wandb.log({\n",
        "            \"cert/final\": cert,\n",
        "            \"cert/best_track_estimate\": best_est_cert,\n",
        "            \"cert/best_epoch\": best_epoch,\n",
        "            \"eval/relL2_mean\": rel_mean,\n",
        "            \"eval/relL2_stoch_avg\": rel_stoch,\n",
        "            \"time/seconds\": elapsed,\n",
        "            \"epoch\": epochs,\n",
        "            **imgs\n",
        "        })\n",
        "        run.finish()\n",
        "\n",
        "    return modelB, {\n",
        "        \"certificate\": cert,\n",
        "        \"relL2_mean\": rel_mean,\n",
        "        \"relL2_stoch\": rel_stoch,\n",
        "        \"time\": elapsed,\n",
        "        \"certificate_track_best\": best_est_cert,\n",
        "        \"best_epoch\": best_epoch,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "YcGHt24VJXAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Sweep utilities\n",
        "# -----------------------\n",
        "def sweep_pinn(cfg: PINNConfig, Nu_list: List[int], Nf_list: List[int]) -> pd.DataFrame:\n",
        "    data = BurgersData(cfg)\n",
        "    rows = []\n",
        "    for Nu in Nu_list:\n",
        "        row = {\"Nu\": Nu}\n",
        "        for Nf in Nf_list:\n",
        "            print(f\"=== [PINN] Nu={Nu}, Nf={Nf} ===\")\n",
        "            cfg_run = PINNConfig(**{**cfg.__dict__})\n",
        "            cfg_run.N_ic = Nu\n",
        "            cfg_run.N_f  = Nf\n",
        "            _, rel, _meta = train_pinn(cfg_run, data, verbose=False)\n",
        "            row[str(Nf)] = rel\n",
        "            print(f\"-> rel-L2={rel:.3e}\")\n",
        "        rows.append(row)\n",
        "    df = pd.DataFrame(rows).set_index(\"Nu\")\n",
        "    return df\n",
        "\n",
        "def sweep_pacbayes(cfg: PINNConfig, Nu_list: List[int], Nf_list: List[int]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    data = BurgersData(cfg)\n",
        "    rows_cert = []\n",
        "    rows_rel = []\n",
        "    for Nu in Nu_list:\n",
        "        row_cert = {\"Nu\": Nu}\n",
        "        row_rel  = {\"Nu\": Nu}\n",
        "        for Nf in Nf_list:\n",
        "            print(f\"=== [PBB] Nu={Nu}, Nf={Nf} ===\")\n",
        "            cfg_run = PINNConfig(**{**cfg.__dict__})\n",
        "            cfg_run.Nu = Nu; cfg_run.Nf = Nf\n",
        "            modelB, res = train_pacbayes(cfg_run, data, verbose=False)\n",
        "            row_cert[str(Nf)] = res[\"certificate\"]\n",
        "            row_rel[str(Nf)]  = res[\"relL2_mean\"]  # you can also store relL2_stoch\n",
        "            print(f\"-> cert={res['certificate']:.3e} relL2_mean={res['relL2_mean']:.3e} relL2_stoch={res['relL2_stoch']:.3e}\")\n",
        "        rows_cert.append(row_cert)\n",
        "        rows_rel.append(row_rel)\n",
        "    df_cert = pd.DataFrame(rows_cert).set_index(\"Nu\")\n",
        "    df_rel  = pd.DataFrame(rows_rel).set_index(\"Nu\")\n",
        "    return df_cert, df_rel"
      ],
      "metadata": {
        "id": "OWNzflskWwTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2ozwYzxVQIS"
      },
      "outputs": [],
      "source": [
        "from dataclasses import asdict\n",
        "# === Colab/Drive helpers & sweep-and-save ===\n",
        "import os, gc, json, pandas as pd, torch, torch.nn as nn\n",
        "\n",
        "def ensure_dir(p):\n",
        "    os.makedirs(p, exist_ok=True); return p\n",
        "\n",
        "def export_posterior_mean_fcnet(pb_model: BayesFCNet, cfg: PINNConfig, path: str):\n",
        "    \"\"\"Build a vanilla FCNet whose weights equal the posterior mean of pb_model and save state_dict().\"\"\"\n",
        "    det = FCNet(width=cfg.hidden_width, depth=cfg.hidden_layers, activation=cfg.activation).to(\"cpu\")\n",
        "    lin = [m for m in det.net if isinstance(m, nn.Linear)]\n",
        "    for i, bayes_layer in enumerate(pb_model.layers):\n",
        "        Wmu, bmu = bayes_layer.mean_params()\n",
        "        lin[i].weight.data.copy_(Wmu.detach().cpu())\n",
        "        lin[i].bias.data.copy_(bmu.detach().cpu())\n",
        "    Wmu, bmu = pb_model.out.mean_params()\n",
        "    lin[-1].weight.data.copy_(Wmu.detach().cpu())\n",
        "    lin[-1].bias.data.copy_(bmu.detach().cpu())\n",
        "    torch.save(det.state_dict(), path)\n",
        "\n",
        "@torch.no_grad()\n",
        "def _gpu_hygiene():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def run_grid_and_save(cfg: PINNConfig,\n",
        "                      data: BurgersData,\n",
        "                      Nu_list, Nf_list,\n",
        "                      outdir_base: str,\n",
        "                      log_wandb: bool = True):\n",
        "    \"\"\"\n",
        "    For each (Nu, Nf):\n",
        "      - run deterministic PINN\n",
        "      - run PAC-Bayes PINN (tracking best bound; restores best snapshot)\n",
        "      - save: best posterior state, posterior-mean deterministic state\n",
        "      - record metrics: relL2_det_baseline, relL2_posterior_mean, relL2_probabilistic, certificate, etc.\n",
        "      - write a single CSV to Google Drive.\n",
        "    \"\"\"\n",
        "    # Prepare output folders\n",
        "    outdir = ensure_dir(outdir_base.rstrip(\"/\"))\n",
        "    ckpt_dir = ensure_dir(os.path.join(outdir, \"checkpoints\"))\n",
        "    mean_dir  = ensure_dir(os.path.join(outdir, \"posterior_mean_det\"))\n",
        "\n",
        "    rows = []\n",
        "    seed = cfg.seed\n",
        "\n",
        "    for Nu in Nu_list:\n",
        "        for Nf in Nf_list:\n",
        "            print(f\"\\n=== RUN (Nu={Nu}, Nf={Nf}, seed={seed}) ===\")\n",
        "            # fresh per-run config (don’t mutate the original)\n",
        "            # AFTER (uses only real dataclass fields)\n",
        "            cfg_run = PINNConfig(**asdict(cfg))\n",
        "            cfg_run.N_ic, cfg_run.N_f = Nu, Nf\n",
        "\n",
        "            # unique filenames\n",
        "            tag = f\"Nu{Nu}_Nf{Nf}_seed{seed}\"\n",
        "            cfg_run.save_best_path = os.path.join(ckpt_dir, f\"pbb_best_state_{tag}.pt\")\n",
        "            mean_path = os.path.join(mean_dir, f\"pbb_posterior_mean_det_{tag}.pt\")\n",
        "            metrics_path_csv = os.path.join(outdir, f\"metrics_{tag}.csv\")\n",
        "            metrics_path_json = os.path.join(outdir, f\"metrics_{tag}.json\")\n",
        "\n",
        "            # --- Deterministic baseline ---\n",
        "            det_model, rel_det, meta_det = train_pinn(cfg_run, data, log_wandb=log_wandb, wb_project=\"pinn-burgers\")\n",
        "            time_det = meta_det.get(\"time\", float(\"nan\"))\n",
        "            # free\n",
        "            del det_model; _gpu_hygiene()\n",
        "\n",
        "            # --- PAC-Bayes ---\n",
        "            pb_model, out_pbb = train_pacbayes(cfg_run, data, log_wandb=log_wandb,\n",
        "                                               wb_project=\"pinn-burgers\",\n",
        "                                               log_every=100, epochs=cfg_run.pbb_epochs)\n",
        "            # export posterior-mean deterministic weights (for full reproducibility of relL2_mean)\n",
        "            export_posterior_mean_fcnet(pb_model, cfg_run, mean_path)\n",
        "\n",
        "            # gather metrics\n",
        "            row = {\n",
        "                \"Nu\": Nu, \"Nf\": Nf, \"seed\": seed,\n",
        "                \"relL2_det_baseline\": rel_det,                    # deterministic PINN baseline\n",
        "                \"relL2_posterior_mean\": out_pbb[\"relL2_mean\"],    # deterministic w/ posterior mean @ best snapshot\n",
        "                \"relL2_probabilistic\": out_pbb[\"relL2_stoch\"],    # stochastic predictor @ best snapshot\n",
        "                \"certificate_final\": out_pbb[\"certificate\"],      # high-MC cert at best snapshot\n",
        "                \"certificate_track_best\": out_pbb.get(\"certificate_track_best\", float(\"nan\")),\n",
        "                \"best_epoch\": out_pbb.get(\"best_epoch\", -1),\n",
        "                \"time_det_sec\": time_det,\n",
        "                \"time_pbb_sec\": out_pbb.get(\"time\", float(\"nan\")),\n",
        "                \"best_state_path\": cfg_run.save_best_path,\n",
        "                \"posterior_mean_det_path\": mean_path,\n",
        "            }\n",
        "            # per-run CSV/JSON (optional but handy)\n",
        "            pd.DataFrame([row]).to_csv(metrics_path_csv, index=False)\n",
        "            with open(metrics_path_json, \"w\") as f:\n",
        "                json.dump(row, f, indent=2)\n",
        "\n",
        "            rows.append(row)\n",
        "\n",
        "            # free\n",
        "            del pb_model; _gpu_hygiene()\n",
        "\n",
        "    # --- Write a single tidy CSV for the whole sweep ---\n",
        "    df = pd.DataFrame(rows)\n",
        "    sweep_csv = os.path.join(outdir, f\"sweep_summary_seed{seed}.csv\")\n",
        "    df.to_csv(sweep_csv, index=False)\n",
        "    print(f\"\\nSaved sweep summary to: {sweep_csv}\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Define your grids\n",
        "Nu_list = [20, 50, 100, 200]\n",
        "Nf_list = [2000, 4000, 6000, 8000, 10000]"
      ],
      "metadata": {
        "id": "IuMIlSKui-Cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = PINNConfig(\n",
        "    Nf_master=max(Nf_list),\n",
        "\n",
        "    # training knobs\n",
        "    adam_epochs=1000, lbfgs_max_iter=200,\n",
        "    pbb_epochs=1000, pbb_lr=1e-4, pbb_momentum=0.95,\n",
        "    pbb_mc_train=1,\n",
        "\n",
        "    # FINAL certificate as in the paper\n",
        "    cert_mc=512,\n",
        "\n",
        "    # tracking bound during training (used for picking the best snapshot)\n",
        "    # you can set this equal to cert_mc if you want \"full\" tracking,\n",
        "    # but 8192–32768 is usually plenty-stable while much cheaper.\n",
        "    cert_track_every=200,\n",
        "    cert_track_mc=16,\n",
        ")\n",
        "data = BurgersData(cfg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_8l0F9SYexn",
        "outputId": "aa5b38cd-39ca-4027-af3c-5e4072d085be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Data] downloading https://raw.githubusercontent.com/maziarraissi/PINNs/master/appendix/Data/burgers_shock.mat -> burgers_shock.mat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Mount Drive (once per runtime)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2) Choose an output directory in Drive\n",
        "outdir = \"/content/drive/MyDrive/pinn_pbb_results/burgers_clean\"\n",
        "\n",
        "# 4) Run\n",
        "df_summary = run_grid_and_save(cfg, data, Nu_list, Nf_list, outdir_base=outdir, log_wandb=True)\n",
        "df_summary.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "6TA7IjplWpN4",
        "outputId": "bcb30cf6-c22e-4c30-cfba-e15b063f0ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-190782253.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1) Mount Drive (once per runtime)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 2) Choose an output directory in Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    }
  ]
}